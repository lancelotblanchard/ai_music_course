{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5: Representation Learning for Music\n",
    "\n",
    "**Agenda:**\n",
    "\n",
    "- Comparison of Music representations\n",
    "- Case Study: Encodec\n",
    "- Hands On 1: Using Encodec\n",
    "- Hands On 2: Encoding MIDI\n",
    "- Hands On 3: Using HiFi-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of some representations\n",
    "\n",
    "From [Comparing Representations for Audio Synthesis Using Generative Adversarial Networks (Nistal, Lattner, Richard, 2020)](https://arxiv.org/abs/2006.09266).\n",
    "\n",
    "![](./assets/comparison_representations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive into Encodec\n",
    "\n",
    "Encodec is a *neural audio compression framework* developed by Meta, that\n",
    "enables the efficient compression of high-fidelity audio into a compact discrete\n",
    "representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/encodec_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Residual Vector Quantization (RVQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/rvq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limitation of this method is that it takes $N_q$ steps (with many lookups!) to\n",
    "quantize one single vector. So this becomes very costly to quantize an entire \n",
    "tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encodec speedup: RVQ through Transformers\n",
    "\n",
    "![](./assets/rvq_transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 1: Using Encodec to encode and decode audio waveforms\n",
    "\n",
    "We can directly use the `EncodecModel` provided by the `transformers` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.abspath(\"../huggingface_hub_cache/\")\n",
    "\n",
    "from transformers import EncodecModel, EncodecFeatureExtractor\n",
    "\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "processor = EncodecFeatureExtractor.from_pretrained(\"facebook/encodec_24khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "# Again, let's get some info using torchinfo\n",
    "summary(model, input_data=[torch.randn(1, 1, 320)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we get this downsampling factor value? We can just multiply all of the\n",
    "# strides!\n",
    "downsampling_factor = ...\n",
    "\n",
    "print(f\"Downsampling factor: {downsampling_factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# Let's load the audio file\n",
    "y, sr = librosa.load(\"../session2_setup/assets/stargazing.wav\", sr=processor.sampling_rate)\n",
    "\n",
    "# Let's first go through the processor\n",
    "inputs = ...\n",
    "\n",
    "# Let's select the lowest bandwidth\n",
    "bandwidth = ...\n",
    "print(f\"Target bandwidth: {bandwidth} kbps\")\n",
    "\n",
    "# We can now pass the inputs to the model to get the encoder outputs\n",
    "encoder_outputs = ...\n",
    "\n",
    "# Let's get the shape of the encoder outputs\n",
    "print(encoder_outputs.audio_codes.shape)\n",
    "\n",
    "# Let's calculate the compression ratio\n",
    "compression_ratio = ...\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's decode those codes back to audio\n",
    "audio_values = ...\n",
    "\n",
    "display(Audio(audio_values.detach().numpy(), rate=processor.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it one more time with a higher bandwidth!\n",
    "bandwidth_high = ...\n",
    "print(f\"Target bandwidth: {bandwidth_high} kbps\")\n",
    "\n",
    "encoder_outputs_high = ...\n",
    "print(f\"Encoder outputs shape: {encoder_outputs_high.audio_codes.shape}\")\n",
    "\n",
    "# Let's decode those codes back to audio\n",
    "audio_values_high = ...\n",
    "\n",
    "display(Audio(audio_values_high.detach().numpy(), rate=processor.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print some of the codes\n",
    "print(encoder_outputs_high.audio_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's dive a bit through the encoder (before the quantization)\n",
    "pre_quantized = ...\n",
    "print(f\"Pre-quantized shape: {pre_quantized.shape}\")\n",
    "print(f\"Hidden size of the encoder: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the outputs of the first codebook and print them\n",
    "cb1 = ...\n",
    "print(cb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How far away are we? Let's decode the first code and compare to our first vector\n",
    "x_hat = ...\n",
    "print(x_hat.shape)\n",
    "\n",
    "# Let's calculate the Euclidean distance between the two vectors\n",
    "dist = ...\n",
    "print(f\"Euclidean distance: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it the best we can do? Let's verify whether `quantize` gave us the best code\n",
    "distances = ...\n",
    "\n",
    "argmin = torch.argmin(torch.tensor(distances))\n",
    "\n",
    "print(f\"Minimum distance is {distances[argmin]:.4f} for code {argmin.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the outputs of the SECOND codebook, and print them.\n",
    "residual = ...\n",
    "\n",
    "cb2 = ...\n",
    "print(cb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check whether we improved the distance\n",
    "x_hat_2 = ...\n",
    "print(x_hat_2.shape)\n",
    "\n",
    "# Let's calculate the Euclidean distance between the two vectors\n",
    "dist = ...\n",
    "print(f\"Euclidean distance: {dist:.4f}\") # hope that this is smaller than the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the final distance\n",
    "n_codebooks = ...\n",
    "all_x_hats = ...\n",
    "\n",
    "# Let's calculate the Euclidean distance between the two vectors\n",
    "final_dist = ...\n",
    "print(f\"Euclidean distance: {final_dist:.4f}\") # hope that this is smaller than the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's plot how distance decreases with higher bandwidth / more codebooks\n",
    "distances = ...\n",
    "\n",
    "target_codebooks = ...\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 2: Encoding MIDI using the Anticipatory Music Transformer strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "from collections import defaultdict\n",
    "\n",
    "# This is our time resolution (how many ticks per second)\n",
    "TIME_RESOLUTION = 100\n",
    "\n",
    "# Let's first load the MIDI file\n",
    "midi = mido.MidiFile(\"../session2_setup/assets/symphony40.mid\")\n",
    "\n",
    "def midi_to_tokens(midi):\n",
    "    ...\n",
    "    pass\n",
    "\n",
    "# We can call our function\n",
    "tokens = midi_to_tokens(midi)\n",
    "print(f\"Length of sequence: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_midi(tokens, bpm=120):\n",
    "    ...\n",
    "    pass\n",
    "\n",
    "# We can call our function\n",
    "reconstructed_midi = tokens_to_midi(tokens)\n",
    "reconstructed_midi.save(\"assets/reconstructed_midi.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import midi2audio\n",
    "\n",
    "# We can listen to our reconstructed MIDI\n",
    "midi2audio_obj = midi2audio.FluidSynth(\"../session2_setup/assets/soundfont.sf2\")\n",
    "midi2audio_obj.midi_to_audio(\"assets/reconstructed_midi.mid\", \"assets/reconstructed_midi.wav\")\n",
    "\n",
    "y, sr = librosa.load(\"assets/reconstructed_midi.wav\")\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 3: Using HiFi-GAN to synthesize speech spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone HiFi-GAN repository\n",
    "!git clone https://github.com/jik876/hifi-gan.git ../repositories/hifi-gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "# Download pretrained model from `lancelotblanchard/hifi_gan_vctk_v3`\n",
    "\n",
    "model_path = huggingface_hub.snapshot_download(\n",
    "    repo_id=\"lancelotblanchard/hifi_gan_vctk_v3\",\n",
    "    cache_dir=\"../huggingface_hub_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the path to the repository to the system path\n",
    "import sys\n",
    "sys.path.append(\"../repositories/hifi-gan\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from env import AttrDict\n",
    "\n",
    "config_file = os.path.join(model_path, \"config.json\")\n",
    "with open(config_file) as f:\n",
    "    h = AttrDict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the config\n",
    "print(json.dumps(h, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator\n",
    "import torch\n",
    "\n",
    "generator = Generator(h)\n",
    "state_dict_g = torch.load(os.path.join(model_path, \"generator_v3\"), map_location=\"cpu\")[\"generator\"]\n",
    "generator.load_state_dict(state_dict_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's grab an audio file\n",
    "y, sr = librosa.load(\"../session2_setup/assets/stargazing.wav\")\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torch.from_numpy(y).reshape(1, -1)\n",
    "\n",
    "n_fft = h[\"n_fft\"]\n",
    "num_mels = h[\"num_mels\"]\n",
    "fmin = h[\"fmin\"]\n",
    "fmax = h[\"fmax\"]\n",
    "win_size = h[\"win_size\"]\n",
    "hop_size = h[\"hop_size\"]\n",
    "\n",
    "# we use librosa's mel() function to create the mel filterbank\n",
    "mel = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
    "mel = torch.from_numpy(mel).float()\n",
    "\n",
    "# We create a Hann window of size win_size.\n",
    "# The Hann window is used to smooth the signal before applying the STFT.\n",
    "hann_window = torch.hann_window(win_size)\n",
    "\n",
    "# We pad the audio signal to make sure that the length is a multiple of hop_size\n",
    "audio = torch.nn.functional.pad(audio, (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)), mode=\"reflect\")\n",
    "audio = audio.squeeze(1)\n",
    "\n",
    "# We compute the STFT of the audio signal\n",
    "spec = torch.view_as_real(torch.stft(audio, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window,\n",
    "                  center=False, pad_mode=\"reflect\", normalized=False, onesided=True, return_complex=True))\n",
    "\n",
    "# We compute the magnitude of the STFT\n",
    "spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n",
    "\n",
    "# Finally, we compute the mel spectrogram by applying the mel filterbank to the\n",
    "# magnitude of the STFT.\n",
    "spec = torch.matmul(mel, spec)\n",
    "\n",
    "# We normalize the mel spectrogram to the range [0, 1]\n",
    "spec = torch.log(torch.clamp(spec, min=1e-5) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reset the matplotlib backend to use the inline backend\n",
    "import matplotlib\n",
    "from importlib import reload\n",
    "matplotlib = reload(matplotlib)\n",
    "matplotlib.use(\"inline\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Let's plot the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(spec[0].detach().numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.title(\"Mel spectrogram\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_g_hat = generator(spec)\n",
    "\n",
    "audio2 = y_g_hat.squeeze()\n",
    "\n",
    "display(Audio(audio2.detach().numpy(), rate=22050))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
