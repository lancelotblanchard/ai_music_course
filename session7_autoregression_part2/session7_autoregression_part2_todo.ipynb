{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 7: Autoregressive Music Generation (Part 2)\n",
    "\n",
    "Agenda\n",
    "- Understanding MusicGen\n",
    "- Hands On: Using MusicGen to generate audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding MusicGen\n",
    "\n",
    "From the paper [Simple and Controllable Music Generation (Copet et al., 2023)](https://arxiv.org/abs/2306.05284).\n",
    "\n",
    "![](./assets/musicgen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On: Using MusicGen to generate audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.abspath(\"../huggingface_hub_cache\")\n",
    "\n",
    "from transformers import MusicgenMelodyForConditionalGeneration, MusicgenMelodyProcessor\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = MusicgenMelodyProcessor.from_pretrained(\"facebook/musicgen-melody\")\n",
    "model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can first generate unconditional music\n",
    "unconditional_inputs = ...\n",
    "\n",
    "unconditional_audio_values = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's listen to our audio\n",
    "\n",
    "display(Audio(unconditional_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also generate a piece of music conditionally, with a given text prompt\n",
    "\n",
    "text_conditioned_inputs = ...\n",
    "\n",
    "text_conditioned_audio_values = ...\n",
    "\n",
    "display(Audio(text_conditioned_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio\n",
    "\n",
    "# And we can also generate with a melody condition, passed as an audio array\n",
    "\n",
    "y, sr = librosa.load(\"bolero_ravel.mp3\", sr=model.config.sampling_rate)\n",
    "\n",
    "display(Audio(y, rate=sr))\n",
    "\n",
    "melody_conditioned_inputs = ...\n",
    "\n",
    "melody_conditioned_audio_values = ...\n",
    "\n",
    "display(Audio(melody_conditioned_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how this model actually generates music\n",
    "\n",
    "# ############### #\n",
    "# 0. CONDITIONING # \n",
    "# ############### #\n",
    "\n",
    "text_prompt = ...\n",
    "inputs_tensor = text_prompt[\"input_ids\"].to(model.device)\n",
    "attention_mask = text_prompt[\"attention_mask\"].to(model.device)\n",
    "\n",
    "print(inputs_tensor)\n",
    "print(attention_mask)\n",
    "\n",
    "# Then, we get our melody conditioning (a chroma spectrogram)\n",
    "melody_prompt = ...\n",
    "input_features = melody_prompt[\"input_features\"].to(model.device)\n",
    "\n",
    "print(melody_prompt[\"input_features\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# ################# #\n",
    "# 1. PREPARE CONFIG #\n",
    "# ################# #\n",
    "\n",
    "generation_config = copy.deepcopy(model.generation_config)\n",
    "model._prepare_special_tokens(generation_config, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# #################### #\n",
    "# 2. TEXT CONDITIONING #\n",
    "# #################### #\n",
    "\n",
    "encoder = model.get_text_encoder()\n",
    "with torch.no_grad():\n",
    "    encoder_hidden_states = ...\n",
    "\n",
    "# project encoder_hidden_states\n",
    "encoder_hidden_states = ...\n",
    "\n",
    "# for classifier free guidance we need to add a 'null' input to our encoder hidden states\n",
    "encoder_hidden_states = ...\n",
    "encoder_attention_mask = ...\n",
    "encoder_hidden_states = encoder_hidden_states * encoder_attention_mask[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################### #\n",
    "# 3. AUDIO CONDITIONING #\n",
    "# ##################### #\n",
    "\n",
    "null_audio_hidden_states = ...\n",
    "\n",
    "# for classifier free guidance we need to add a 'null' input to our audio hidden states\n",
    "audio_hidden_states = torch.concatenate([input_features, null_audio_hidden_states], dim=0)\n",
    "\n",
    "# project audio_hidden_states ->\n",
    "# (batch_size, seq_len, num_chroma) -> (batch_size, seq_len, hidden_size)\n",
    "audio_hidden_states = ...\n",
    "\n",
    "# pad or truncate to config.chroma_length\n",
    "n_repeat = ...\n",
    "audio_hidden_states = ...\n",
    "\n",
    "audio_hidden_states = ...\n",
    "\n",
    "encoder_hidden_states = torch.cat([audio_hidden_states, encoder_hidden_states], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################### #\n",
    "# 4. PREPARE AUTO-REGRESSIVE GENERATION #\n",
    "# ##################################### #\n",
    "\n",
    "input_ids = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################### #\n",
    "# 5. BUILD DELAY PATTERN #\n",
    "# ###################### #\n",
    "\n",
    "max_length = 513\n",
    "\n",
    "input_ids, decoder_delay_pattern_mask =...\n",
    "print(decoder_delay_pattern_mask.shape)\n",
    "print(decoder_delay_pattern_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ClassifierFreeGuidanceLogitsProcessor, LogitsProcessorList, TopKLogitsWarper\n",
    "\n",
    "# ########################### #\n",
    "# 6. PREPARE LOGITS PROCESSOR #\n",
    "# ########################### #\n",
    "\n",
    "guidance_scale = 3\n",
    "\n",
    "logits_processor = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# ############################ #\n",
    "# 7. PREPARE STOPPING CRITERIA #\n",
    "# ############################ #\n",
    "\n",
    "stopping_criteria = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### #\n",
    "# 8. RUN SAMPLING LOOP #\n",
    "# #################### #\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################ #\n",
    "# 9. DECODE OUTPUT #\n",
    "# ################ #\n",
    "\n",
    "# apply the pattern mask to the final ids\n",
    "output_ids = ...\n",
    "\n",
    "# revert the pattern delay mask by filtering the pad token id\n",
    "output_ids = ...\n",
    "\n",
    "# append the frame dimension back to the audio codes\n",
    "output_ids = ...\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_values = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we get a similar output?\n",
    "\n",
    "display(Audio(output_values.cpu().squeeze(0, 1), rate=model.config.sampling_rate))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
