{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6: Autoregressive Music Generation\n",
    "\n",
    "Agenda\n",
    "- Overview of the Transformer model\n",
    "- Understanding Anticipatory Music Transformers\n",
    "- Understanding MusicGen\n",
    "- Hands On 1: Using AMT to generate MIDI data\n",
    "- Hands On 2: Using MusicGen to generate audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer architecture\n",
    "\n",
    "From the paper [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "![](./assets/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Anticipatory Music Transformers\n",
    "\n",
    "From the papers [Music Transformer (Huang et al., 2018)](https://arxiv.org/abs/1809.04281) and [Anticipatory Music Transformers (Thickstun et al., 2023)](https://arxiv.org/abs/2306.08620).\n",
    "\n",
    "![](./assets/music_transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding MusicGen\n",
    "\n",
    "From the paper [Simple and Controllable Music Generation (Copet et al., 2023)](https://arxiv.org/abs/2306.05284).\n",
    "\n",
    "![](./assets/musicgen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On 1: Using AMT to generate MIDI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the anticipation repository\n",
    "!git clone https://github.com/lancelotblanchard/anticipation.git ../repositories/anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T23:42:56.601205Z",
     "start_time": "2025-05-13T23:42:46.257855Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.abspath(\"../huggingface_hub_cache/\")\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# set Hub Cache folder\n",
    "model = GPT2LMHeadModel.from_pretrained(\"stanford-crfm/music-small-800k\", attn_implementation=\"eager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our repository to the Python path\n",
    "\n",
    "import sys\n",
    "sys.path.append('../repositories/anticipation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anticipation import ops\n",
    "from anticipation.config import MAX_INSTR, MAX_PITCH\n",
    "from anticipation.vocab import ANTICIPATE, CONTROL_OFFSET, DUR_OFFSET, NOTE_OFFSET, TIME_OFFSET\n",
    "from anticipation.sample import nucleus\n",
    "import torch\n",
    "\n",
    "# Let's look at generating some tokens unconditionally. Before we can do so,\n",
    "# we need to look at building an inference function:\n",
    "\n",
    "def generate_note(model, tokens, current_time, active_instruments, top_p=0.98, history_length=340, monophony=False):\n",
    "    assert len(tokens) % 3 == 0 # we need to have a valid sequence\n",
    "\n",
    "    new_tokens = []\n",
    "\n",
    "    ...\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to generate a few notes\n",
    "\n",
    "tokens = []\n",
    "current_time = 0\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anticipation.convert import events_to_midi\n",
    "import midi2audio\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Let's listen to our sequence of tokens\n",
    "\n",
    "# We first need to convert our tokens to a MIDI file\n",
    "events_to_midi(tokens).save(\"assets/generation.mid\")\n",
    "\n",
    "midi2audio_obj = midi2audio.FluidSynth(\"../session2_setup/assets/soundfont.sf2\")\n",
    "midi2audio_obj.midi_to_audio(\"assets/generation.mid\", \"assets/generation.wav\")\n",
    "\n",
    "y, sr = librosa.load(\"assets/generation.wav\", sr=44100)\n",
    "\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's take a look at what logits look like\n",
    "tokens_subset = tokens[:3*10]\n",
    "with torch.no_grad():\n",
    "    input_sequence = ...\n",
    "    logits = ...\n",
    "\n",
    "print(logits.shape)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anticipation.config import MAX_TIME\n",
    "\n",
    "# Let's do the same thing after nucleus sampling processing\n",
    "\n",
    "print(f\"Last time of the sequence is {tokens_subset[-3]}\")\n",
    "\n",
    "# Safety filtering for time tokens\n",
    "...\n",
    "\n",
    "# Get the probability distribution of the new token\n",
    "nucleus_logits = ...\n",
    "probs = ...\n",
    "new_token = ...\n",
    "\n",
    "print(f\"New token would be sampled for {new_token}\")\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anticipation.convert import midi_to_events\n",
    "\n",
    "# Let's now work with a conditioning signal! We'll convert our MIDI file to tokens\n",
    "\n",
    "symphony40 = midi_to_events(\"../session2_setup/assets/symphony40.mid\")\n",
    "print(f\"Number of tokens: {len(symphony40)}, number of notes: {len(symphony40)//3}\")\n",
    "print(f\"First tokens: {symphony40[:6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can calculate the number of instruments by looking at the tokens\n",
    "\n",
    "instruments = set()\n",
    "...\n",
    "\n",
    "print(f\"Number of instruments: {len(instruments)}\")\n",
    "print(f\"Instruments: {instruments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter the first 238 notes of the sequence and only keep instrument 42\n",
    "# We will also keep the notes of instrument 40 as a ground truth\n",
    "\n",
    "control_tokens = []\n",
    "ground_truth = []\n",
    "...\n",
    "\n",
    "events_to_midi(control_tokens).save(\"assets/control.mid\")\n",
    "midi2audio_obj.midi_to_audio(\"assets/control.mid\", \"assets/control.wav\")\n",
    "\n",
    "y, sr = librosa.load(\"assets/control.wav\", sr=44100)\n",
    "\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anticipation.config import DELTA\n",
    "import math\n",
    "\n",
    "# We will use this sequence as a conditioning signal for our generation\n",
    "# Let's generate instrument 42 with the given control signal\n",
    "\n",
    "# First, let's add CONTROL_OFFSET to the control tokens\n",
    "anticipated_control_tokens = ...\n",
    "\n",
    "# We select the first control token and leave the rest for later\n",
    "atime, adur, anote = ...\n",
    "atokens = ...\n",
    "# This is the time of the first control\n",
    "anticipated_time = ...\n",
    "\n",
    "# We will generate until end time\n",
    "end_time = max(control_tokens[::3])\n",
    "current_time = 0\n",
    "conditioned_tokens = []\n",
    "\n",
    "# Generation loop\n",
    "while current_time < end_time:\n",
    "    # Anticipated if needed\n",
    "    while current_time >= anticipated_time - DELTA:\n",
    "        ...\n",
    "\n",
    "    new_tokens = ...\n",
    "    print(new_tokens)\n",
    "    ...\n",
    "\n",
    "# We remove the control tokens and add them without CONTROL_OFFSET\n",
    "conditioned_tokens, _ = ops.split(conditioned_tokens)\n",
    "conditioned_tokens = ops.sort(conditioned_tokens + control_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now listen to our result\n",
    "\n",
    "events_to_midi(conditioned_tokens).save(\"assets/conditioned_generation.mid\")\n",
    "\n",
    "midi2audio_obj = midi2audio.FluidSynth(\"../session2_setup/assets/soundfont.sf2\")\n",
    "midi2audio_obj.midi_to_audio(\"assets/conditioned_generation.mid\", \"assets/conditioned_generation.wav\")\n",
    "\n",
    "y, sr = librosa.load(\"assets/conditioned_generation.wav\", sr=44100)\n",
    "\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 2: Using MusicGen to generate audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T23:44:21.981930Z",
     "start_time": "2025-05-13T23:44:18.530912Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.abspath(\"../huggingface_hub_cache\")\n",
    "\n",
    "from transformers import MusicgenMelodyForConditionalGeneration, MusicgenMelodyProcessor\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = MusicgenMelodyProcessor.from_pretrained(\"facebook/musicgen-melody\")\n",
    "model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can first generate unconditional music\n",
    "unconditional_inputs = ...\n",
    "\n",
    "unconditional_audio_values = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T23:52:27.692067Z",
     "start_time": "2025-05-13T23:49:52.787457Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's listen to our audio\n",
    "\n",
    "display(Audio(unconditional_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also generate a piece of music conditionally, with a given text prompt\n",
    "\n",
    "text_conditioned_inputs = ...\n",
    "\n",
    "text_conditioned_audio_values = ...\n",
    "\n",
    "display(Audio(text_conditioned_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio\n",
    "\n",
    "# And we can also generate with a melody condition, passed as an audio array\n",
    "\n",
    "y, sr = librosa.load(\"bolero_ravel.mp3\", sr=model.config.sampling_rate)\n",
    "\n",
    "display(Audio(y, rate=sr))\n",
    "\n",
    "melody_conditioned_inputs = ...\n",
    "\n",
    "melody_conditioned_audio_values = ...\n",
    "\n",
    "display(Audio(melody_conditioned_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how this model actually generates music\n",
    "\n",
    "# ############### #\n",
    "# 0. CONDITIONING # \n",
    "# ############### #\n",
    "\n",
    "text_prompt = ...\n",
    "inputs_tensor = text_prompt[\"input_ids\"].to(model.device)\n",
    "attention_mask = text_prompt[\"attention_mask\"].to(model.device)\n",
    "\n",
    "print(inputs_tensor)\n",
    "print(attention_mask)\n",
    "\n",
    "# Then, we get our melody conditioning (a chroma spectrogram)\n",
    "melody_prompt = ...\n",
    "input_features = melody_prompt[\"input_features\"].to(model.device)\n",
    "\n",
    "print(melody_prompt[\"input_features\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# ################# #\n",
    "# 1. PREPARE CONFIG #\n",
    "# ################# #\n",
    "\n",
    "generation_config = copy.deepcopy(model.generation_config)\n",
    "model._prepare_special_tokens(generation_config, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# #################### #\n",
    "# 2. TEXT CONDITIONING #\n",
    "# #################### #\n",
    "\n",
    "encoder = model.get_text_encoder()\n",
    "with torch.no_grad():\n",
    "    encoder_hidden_states = ...\n",
    "\n",
    "# project encoder_hidden_states\n",
    "encoder_hidden_states = ...\n",
    "\n",
    "# for classifier free guidance we need to add a 'null' input to our encoder hidden states\n",
    "encoder_hidden_states = ...\n",
    "encoder_attention_mask = ...\n",
    "encoder_hidden_states = encoder_hidden_states * encoder_attention_mask[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################### #\n",
    "# 3. AUDIO CONDITIONING #\n",
    "# ##################### #\n",
    "\n",
    "null_audio_hidden_states = ...\n",
    "\n",
    "# for classifier free guidance we need to add a 'null' input to our audio hidden states\n",
    "audio_hidden_states = torch.concatenate([input_features, null_audio_hidden_states], dim=0)\n",
    "\n",
    "# project audio_hidden_states ->\n",
    "# (batch_size, seq_len, num_chroma) -> (batch_size, seq_len, hidden_size)\n",
    "audio_hidden_states = ...\n",
    "\n",
    "# pad or truncate to config.chroma_length\n",
    "n_repeat = ...\n",
    "audio_hidden_states = ...\n",
    "\n",
    "audio_hidden_states = ...\n",
    "\n",
    "encoder_hidden_states = torch.cat([audio_hidden_states, encoder_hidden_states], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################### #\n",
    "# 4. PREPARE AUTO-REGRESSIVE GENERATION #\n",
    "# ##################################### #\n",
    "\n",
    "input_ids = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################### #\n",
    "# 5. BUILD DELAY PATTERN #\n",
    "# ###################### #\n",
    "\n",
    "max_length = 513\n",
    "\n",
    "input_ids, decoder_delay_pattern_mask =...\n",
    "print(decoder_delay_pattern_mask.shape)\n",
    "print(decoder_delay_pattern_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ClassifierFreeGuidanceLogitsProcessor, LogitsProcessorList, TopKLogitsWarper\n",
    "\n",
    "# ########################### #\n",
    "# 6. PREPARE LOGITS PROCESSOR #\n",
    "# ########################### #\n",
    "\n",
    "guidance_scale = 3\n",
    "\n",
    "logits_processor = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# ############################ #\n",
    "# 7. PREPARE STOPPING CRITERIA #\n",
    "# ############################ #\n",
    "\n",
    "stopping_criteria = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### #\n",
    "# 8. RUN SAMPLING LOOP #\n",
    "# #################### #\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################ #\n",
    "# 9. DECODE OUTPUT #\n",
    "# ################ #\n",
    "\n",
    "# apply the pattern mask to the final ids\n",
    "output_ids = ...\n",
    "\n",
    "# revert the pattern delay mask by filtering the pad token id\n",
    "output_ids = ...\n",
    "\n",
    "# append the frame dimension back to the audio codes\n",
    "output_ids = ...\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_values = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we get a similar output?\n",
    "\n",
    "display(Audio(output_values.cpu().squeeze(0, 1), rate=model.config.sampling_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
